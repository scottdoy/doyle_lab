{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This basic guide to CNNs is written off of the official PyTorch examples here: \n",
    "- [https://github.com/pytorch/examples/tree/master/mnist](https://github.com/pytorch/examples/tree/master/mnist). \n",
    "- [https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "# Training and testing batch size\n",
    "args[\"train_batch_size\"] = 8 # 64\n",
    "args[\"test_batch_size\"] = 8 # 1000\n",
    "\n",
    "# How long to train for\n",
    "args[\"epochs\"] = 2 # 100\n",
    "\n",
    "# Learning rate: \"Speed\" with which the optimizer adjusts weights\n",
    "args[\"lr\"] = 0.01\n",
    "\n",
    "# Momentum: How quickly the weights respond to changing gradients\n",
    "args[\"momentum\"] = 0.5\n",
    "\n",
    "# Whether to use CUDA or not\n",
    "args[\"no_cuda\"] = True\n",
    "\n",
    "# Seed for reproducible training\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "# How often to spit out log / progress updates\n",
    "args[\"log_interval\"] = 10\n",
    "\n",
    "# Whether to save the trained model\n",
    "args[\"save_model\"] = False\n",
    "\n",
    "# Decide whether to use CUDA\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "\n",
    "# Set the seed\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "# Select the device to use based on the `use_cuda` flag\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Keyword arguments for the dataloader\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "cifar10_trainset = datasets.CIFAR10(root='../data', train=True,\n",
    "                                    download=True, transform=cifar10_transform)\n",
    "cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=args['train_batch_size'],\n",
    "                                                  shuffle=True, num_workers=2)\n",
    "\n",
    "cifar10_testset = datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=cifar10_transform)\n",
    "cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=args['test_batch_size'],\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(images):\n",
    "    img_grid = utils.make_grid(images)\n",
    "    img = img_grid / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random training images (one iteration of the dataloader)\n",
    "dataiter = iter(cifar10_trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(images)\n",
    "\n",
    "# Print the associated labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(args['train_batch_size'])))\n",
    "print(' ')\n",
    "print('The size of the image batch is: {}'.format(images.shape))\n",
    "print('This represents (batch_size, channels, height, width)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Net(nn.Module):\n",
    "    def __init__(self, disp_size):\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        \n",
    "        # Flag whether or not to print out information about the tensor\n",
    "        self.disp_size = disp_size\n",
    "        \n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        \n",
    "        # nn.MaxPool2d(kernel_size, stride)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # nn.Linear(in_features, out_features)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.disp_size:\n",
    "            print('x input size:\\t\\t\\t{}'.format(x.shape))\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        if self.disp_size:\n",
    "            print('x after first block:\\t\\t{}'.format(x.shape))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        if self.disp_size:\n",
    "            print('x after second block:\\t\\t{}'.format(x.shape))\n",
    "\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        if self.disp_size:\n",
    "            print('x after reshape:\\t\\t{}'.format(x.shape))\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.disp_size:\n",
    "            print('x after first linear layer:\\t{}'.format(x.shape))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.disp_size:\n",
    "            print('x after second linear layer:\\t{}'.format(x.shape))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        if self.disp_size:\n",
    "            print('x after third linear layer:\\t{}'.format(x.shape))\n",
    "            print(' ')\n",
    "        return x\n",
    "    \n",
    "cifar10_net = CIFAR10Net(disp_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = cifar10_net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_features = nn.Sequential(*list(cifar10_net.children())[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = cifar10_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = 1\n",
    "output_numpy = outputs[target_img].detach().numpy()\n",
    "#print(output_numpy.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "ax[0].imshow(output_numpy[0,:,:])\n",
    "ax[1].imshow(np.transpose(images[target_img].numpy() / 2 + 0.5, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_net = CIFAR10Net(disp_size=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifar10_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(cifar10_trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cifar10_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(cifar10_testloader)\n",
    "images, labels = dataiter.next()\n",
    "outputs = cifar10_net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(images)\n",
    "print('GroundTruth: ', ' '.join('\\t%5s' % classes[labels[j]] for j in range(args['test_batch_size'])))\n",
    "print('Predicted: ', ' '.join('\\t%5s' % classes[predicted[j]]\n",
    "                              for j in range(args['test_batch_size'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in cifar10_testloader:\n",
    "        images, labels = data\n",
    "        outputs = cifar10_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in cifar10_testloader:\n",
    "        images, labels = data\n",
    "        outputs = cifar10_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the dataset\n",
    "# See: https://forums.fast.ai/t/image-normalization-in-pytorch/7534/7\n",
    "\n",
    "transform = transforms.Compose([\n",
    "#    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(datasets.ImageFolder('./data/tcv_snakes/train', transform=transform), \n",
    "                                         batch_size=4096, shuffle=False)\n",
    "\n",
    "pop_mean = []\n",
    "pop_std0 = []\n",
    "pop_std1 = []\n",
    "\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    numpy_image = data[0].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std0 = np.std(numpy_image, axis=(0,2,3))\n",
    "    batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n",
    "    \n",
    "    pop_mean.append(batch_mean)\n",
    "    pop_std0.append(batch_std0)\n",
    "    pop_std1.append(batch_std1)\n",
    "\n",
    "# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "pop_mean = np.array(pop_mean).mean(axis=0)\n",
    "pop_std0 = np.array(pop_std0).mean(axis=0)\n",
    "pop_std1 = np.array(pop_std1).mean(axis=0)\n",
    "\n",
    "print('Calculated Mean: {}'.format(pop_mean))\n",
    "print('Calculated STD: {}'.format(pop_std0))\n",
    "print('Calculated STD (adjusted): {}'.format(pop_std1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "#    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=pop_mean, std=pop_std0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('./data/tcv_snakes/train',\n",
    "                         transform=transform),\n",
    "    batch_size=args[\"batch_size\"], shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('./data/tcv_snakes/test',\n",
    "                         transform=transform),\n",
    "    batch_size=args[\"test_batch_size\"], shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = inception().to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args[\"lr\"], momentum=args[\"momentum\"])\n",
    "\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args[\"save_model\"]):\n",
    "    torch.save(model.state_dict(), \"cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
