{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3Nd5ZRGid6o"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnKKJI3JigyO"
   },
   "source": [
    "This notebook is intended to demonstrate the use of PyTables for organizing and preparing PyTorch datasets. \n",
    "\n",
    "Useful Links:\n",
    "- [http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html](http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html)\n",
    "- [https://github.com/choosehappy/PytorchDigitalPathology/blob/master/classification_lymphoma_densenet/make_hdf5.ipynb](https://github.com/choosehappy/PytorchDigitalPathology/blob/master/classification_lymphoma_densenet/make_hdf5.ipynb)\n",
    "- [http://www.andrewjanowczyk.com/digital-pathology-classification-using-pytorch-densenet/](http://www.andrewjanowczyk.com/digital-pathology-classification-using-pytorch-densenet/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demonstration, we will walk through an example of a semantic segmentation dataset consisting of **images** and **binary masks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCEnxgsZ22HS"
   },
   "source": [
    "# Imports and Workspace Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiOp-LEei2Xx"
   },
   "source": [
    "We must first define the structure of the datasets that we want to work with. In this case, we have a set of data for nuclei detection, which consists of an H&E image and its associated nuclei labelmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUtIETalpxTO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tables\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import model_selection\n",
    "import sklearn.feature_extraction.image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRkmdLX34Ay8"
   },
   "outputs": [],
   "source": [
    "# Name of the database, used to save the .pytables file\n",
    "dataset_name = \"nuclei\"\n",
    "\n",
    "# Tiles will be pulled from the ROI images; this is the size of the tiles\n",
    "# to extract and save in the database, must be >= to training size\n",
    "patch_size = 1000\n",
    "\n",
    "# Distance to skip between tiles.\n",
    "# 1 = pixel wise extraction\n",
    "# patch_size = non-overlapping tiles\n",
    "stride_size = 250 \n",
    "\n",
    "# Number of pixels to pad *after* resize to image with by mirroring \n",
    "# This ensures that the edges of the tiles will be analyzed properly\n",
    "# ---Note---\n",
    "# One should likely make sure that  (nrow + mirror_pad_size) mod patch_size == 0, \n",
    "# where nrow is the number of rows after resizing\n",
    "# so that no pixels are lost (any remainer is ignored)\n",
    "mirror_pad_size = 250\n",
    "\n",
    "# what percentage of the dataset should be used as a held out validation/testing set\n",
    "test_set_size = 0.1\n",
    "\n",
    "# Ratio to resize input images\n",
    "# 1: No resizing\n",
    "# 0.5: Reduce size by half\n",
    "# 2: Make the image 2x the size\n",
    "resize = 1\n",
    "\n",
    "# Class labels, as recorded on the mask PNGs (?)\n",
    "# TODO: Edit this for annotation formatting (RGB or Index)\n",
    "classes = [0,255] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wXO-HcGp4EXs",
    "outputId": "e71d110f-560b-42f3-f6bf-6c56101bdec0"
   },
   "outputs": [],
   "source": [
    "# Get a random seed so that we can reproducibly do the cross validation setup\n",
    "seed = random.randrange(sys.maxsize)\n",
    "\n",
    "# Set the seed\n",
    "random.seed(seed)\n",
    "\n",
    "print(f\"random seed (note down for reproducibility): {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQPU3DIyp9jG"
   },
   "source": [
    "# Define Data Sources\n",
    "\n",
    "Here we create pointers to data sources (images and masks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZPEa-VOQtExQ",
    "outputId": "fd97715b-a921-43ab-d4d4-76a10c905330"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "img_dir = os.path.join('data', 'nuclei_segmentation', 'images')\n",
    "img_ext = '.jpg'\n",
    "mask_dir = os.path.join('data', 'nuclei_segmentation', 'masks')\n",
    "mask_ext = '.png'\n",
    "\n",
    "# Create a list of the files, in this case we're only\n",
    "# interested in files which have masks so we can use supervised learning\n",
    "img_files = glob.glob(os.path.join(mask_dir, '*' + mask_ext))\n",
    "\n",
    "print(f\"Found {len(img_files)} mask files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlISIOQk4JUM"
   },
   "outputs": [],
   "source": [
    "# Create training and validation stages and split the files appropriately between them\n",
    "phases = {}\n",
    "phases[\"train\"], phases[\"val\"] = next(iter(model_selection.ShuffleSplit(n_splits=1,test_size=test_set_size).split(img_files)))\n",
    "\n",
    "# Specify that we'll be saving 2 different image types to the database\n",
    "# an image and its associated mask\n",
    "imgtypes = [\"img\", \"mask\"]\n",
    "\n",
    "print(f\"Training set size: {len(phases['train'])}\")\n",
    "print(phases['train'])\n",
    "print()\n",
    "print(f\"Validation set size: {len(phases['val'])}\")\n",
    "print(phases['val'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaHfuONyzag8"
   },
   "source": [
    "# Define PyTables Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aapIMiBzdMM"
   },
   "source": [
    "Here, we define a few characteristics of the images that are stored as columns in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3RyK5mJ4G7j"
   },
   "outputs": [],
   "source": [
    "# dtype in which the images will be saved, this indicates\n",
    "# that images will be saved as unsigned int 8 bit, i.e., [0,255]\n",
    "img_dtype = tables.UInt8Atom()\n",
    "\n",
    "# Image filename of the source image\n",
    "img_filename = tables.StringAtom(itemsize=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntVN_7O3u1Yb"
   },
   "outputs": [],
   "source": [
    "# Holder for pytables\n",
    "storage = {}\n",
    "\n",
    "# Block shape specifies what we'll be saving into the pytable array.\n",
    "# Here we assume that masks are 1d and images are 3d\n",
    "block_shape = {}\n",
    "block_shape[\"img\"] = np.array((patch_size, patch_size, 3))\n",
    "block_shape[\"mask\"] = np.array((patch_size, patch_size)) \n",
    "\n",
    "# We can also specify filters, such as compression, to improve storage speed\n",
    "filters = tables.Filters(complevel=6, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fpEjpj3zptm"
   },
   "source": [
    "# Create PyTables Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "a9fwharo4Ne6",
    "outputId": "b11d6531-2eee-4ad3-9d20-64b0252c0d1c"
   },
   "outputs": [],
   "source": [
    "# Create separate records for each phase (training and validation)\n",
    "for phase in phases.keys():\n",
    "    print(f\"Processing data from {phase} phase.\")\n",
    "\n",
    "    # We can keep counts of all the classes for training, since we \n",
    "    # can later use this information to create {better weights}\n",
    "    totals = np.zeros((2, len(classes)))\n",
    "    totals[0,:] = classes\n",
    "  \n",
    "    # Open the respective pytable relative to current working_dir\n",
    "    hdf5_file = tables.open_file(os.path.join('data', 'nuclei_segmentation', f\"{dataset_name}_{phase}.pytable\"), mode='w')\n",
    "\n",
    "    # Create the array for storage\n",
    "    storage[\"filename\"] = hdf5_file.create_earray(hdf5_file.root, 'filename', img_filename, (0,))\n",
    "\n",
    "    # For each of the image types, in this case mask and image, we need to create the associated earray\n",
    "    for imgtype in imgtypes:\n",
    "        storage[imgtype] = hdf5_file.create_earray(hdf5_file.root, imgtype, img_dtype,  \n",
    "                                                   shape=np.append([0], block_shape[imgtype]),\n",
    "                                                   chunkshape=np.append([1], block_shape[imgtype]),\n",
    "                                                   filters=filters)\n",
    "\n",
    "    # Now for each of the files\n",
    "    for fileidx in phases[phase]:\n",
    "        fname = img_files[fileidx] \n",
    "        print(fname)\n",
    "\n",
    "        for imgtype in imgtypes:\n",
    "            # if we're looking at an img, it must be 3 channel, but cv2 won't load \n",
    "            # it in the correct channel order, so we need to fix that\n",
    "            if(imgtype==\"img\"):\n",
    "                #io=cv2.cvtColor(cv2.imread('data/imgs/'+os.path.basename(fname).replace(\"_mask.png\",\".tif\")), cv2.COLOR_BGR2RGB)\n",
    "                io = np.array(Image.open(os.path.join(img_dir, os.path.basename(fname).replace(\"_mask\"+mask_ext, img_ext))))\n",
    "                interp_method=PIL.Image.BICUBIC\n",
    "                \n",
    "                # Apply the padding specified in the parameters\n",
    "                # Need to check that this works for \n",
    "                io = np.pad(io, [(mirror_pad_size, mirror_pad_size), (mirror_pad_size, mirror_pad_size), (0, 0)], mode=\"reflect\")\n",
    "\n",
    "                patch_extraction_size = (patch_size,patch_size,3)\n",
    "            # If its a mask image, then we only need a single channel \n",
    "            # (since grayscale 3D images are equal in all channels)\n",
    "            else:\n",
    "                # the image is loaded as {0,255}, \n",
    "                # but we'd like to store it as {0,1} since this represents the binary nature of the mask easier\n",
    "                #io = cv2.imread(fname)/255\n",
    "                io = np.array(Image.open(fname))\n",
    "#                 print(f\"Image mask maximum: {np.max(np.array(io))}\")\n",
    "#                 print(f\"Image mask size: {np.shape(io)}\")\n",
    "                \n",
    "                # Want to use nearest! otherwise resizing \n",
    "                # may cause non-existing classes to be produced via interpolation (e.g., \".25\")\n",
    "                interp_method = PIL.Image.NEAREST\n",
    "\n",
    "                # sum the number of pixels, this is done pre-resize, \n",
    "                # the but proportions don't change which is really what we're after\n",
    "                for i,key in enumerate(classes):\n",
    "                    totals[1,i] += sum(sum(io[:,:]==1))\n",
    "                \n",
    "                # Apply the padding specified in the parameters\n",
    "                # Need to check that this works for \n",
    "                io = np.pad(io, [(mirror_pad_size, mirror_pad_size), (mirror_pad_size, mirror_pad_size)], mode=\"reflect\")\n",
    "                patch_extraction_size = (patch_size, patch_size)\n",
    "\n",
    "            # Resize the image, if desired\n",
    "            #       io = cv2.resize(io,(0,0),fx=resize,fy=resize, interpolation=interp_method) #resize it as specified above\n",
    "#             if resize != 1:\n",
    "#                 io = io.resize((resize,resize),Image.BILINEAR)\n",
    "\n",
    "#             io = np.array(io)\n",
    "            \n",
    "            \n",
    "            #convert input image into overlapping tiles, size is ntiler x ntilec x 1 x patch_size x patch_size x3\n",
    "            io_arr_out = sklearn.feature_extraction.image.extract_patches(io, patch_extraction_size, stride_size)\n",
    "\n",
    "            #resize it into a ntile x patch_size x patch_size x 3\n",
    "            if imgtype == \"img\":\n",
    "                io_arr_out = io_arr_out.reshape(-1,patch_size,patch_size,3)\n",
    "            else:\n",
    "                io_arr_out = io_arr_out.reshape(-1,patch_size,patch_size)\n",
    "\n",
    "            #save the 4D tensor to the table\n",
    "            if(imgtype==\"img\"):\n",
    "                storage[imgtype].append(io_arr_out)\n",
    "            else:\n",
    "                storage[imgtype].append(io_arr_out.squeeze()) #only need 1 channel for mask data\n",
    "\n",
    "        storage[\"filename\"].append([fname for x in range(io_arr_out.shape[0])]) #add the filename to the storage array\n",
    "\n",
    "    # lastely, we should store the number of pixels\n",
    "    npixels = hdf5_file.create_carray(hdf5_file.root, 'numpixels', tables.Atom.from_dtype(totals.dtype), totals.shape)\n",
    "    npixels[:] = totals\n",
    "    hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTables Demonstration for PyTorch.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
