{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This codebook contains sets of Python that you can cut-and-paste for data analysis and visualization projects.\n",
    "\n",
    "For the dataset we are using the BCA Wisconsin dataset.\n",
    "It is a `csv` formatted file containing morphological and image features of nuclei for breast cancer fine-needle aspirates.\n",
    "Each sample is labeled as \"benign\" or \"malignant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "For this codebook we put some generic imports at the top, and then each section will have its own imports depending on where it gets used. \n",
    "This means there may be some things that are imported more than is strictly necessary if the notebook is run top to bottom; however, it should make it easier to cut and paste later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Python imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Standard Scientific Python imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "We typically load data from `.csv` files, which are basically spreadsheets.\n",
    "\n",
    "In Excel, you can export spreadsheets as `.csv` by going to \"File\" -> \"Export\" -> \"Change File Type\" and selecting **\"CSV (Comma delimited) (*.csv)**\" under \"Other File Types\".\n",
    "\n",
    "When loading data into Python for data analysis, it's pretty common to use [Pandas](https://pandas.pydata.org/).\n",
    "Here are some resources:\n",
    "\n",
    "- [Statistical Data Analysis in Python](https://www.kdnuggets.com/2016/07/statistical-data-analysis-python.html)\n",
    "    - [Introduction to Pandas](https://nbviewer.jupyter.org/urls/gist.github.com/fonnesbeck/5850375/raw/c18cfcd9580d382cb6d14e4708aab33a0916ff3e/1.+Introduction+to+Pandas.ipynb)\n",
    "    - [Data Wrangling with Pandas](https://nbviewer.jupyter.org/urls/gist.github.com/fonnesbeck/5850413/raw/3a9406c73365480bc58d5e75bc80f7962243ba17/2.+Data+Wrangling+with+Pandas.ipynb)\n",
    "    - [Plotting and Visualization](https://nbviewer.jupyter.org/urls/gist.github.com/fonnesbeck/5850463/raw/a29d9ffb863bfab09ff6c1fc853e1d5bf69fe3e4/3.+Plotting+and+Visualization.ipynb)\n",
    "    - [Statistical Data Modeling](https://nbviewer.jupyter.org/urls/gist.github.com/fonnesbeck/5850483/raw/5e049b2fdd1c83ae386aa3205d3fc50a1a05e5b0/4.+Statistical+Data+Modeling.ipynb)\n",
    "- [Official Documentation: 10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n",
    "- [Pandas Cheatsheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.path.join to construct the file path\n",
    "# Keeps things nice when going between Windows / non-Windows platforms\n",
    "csv_file = os.path.join('data', 'bca_wisconsin', 'bca_wisconsin.csv')\n",
    "\n",
    "# Use `read_csv` to get the file into a dataframe\n",
    "data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Exploration\n",
    "\n",
    "The order of data wrangling techniques is debatable, but number one is **separate out a testing set**.\n",
    "\n",
    "## Train-Test Splitting\n",
    "\n",
    "Rule of thumb: **60-70%** of the data for training\n",
    "\n",
    "- **Stratified** sampling: Maintain the overall label distribution in your training and testing sets\n",
    "- **Random** sampling: Ignore labels in splitting up the data\n",
    "\n",
    "If you have labels, then perform stratified labeling as described below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create the splitting object\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply the split to the data frame using the \"diagnosis\" column as our label\n",
    "for train_index, test_index in split.split(data_frame, data_frame[\"diagnosis\"]):\n",
    "    train_set = data_frame.loc[train_index]\n",
    "    test_set = data_frame.loc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data has been split appropriately\n",
    "print('Overall class ratio:')\n",
    "print('{}'.format(data_frame[\"diagnosis\"].value_counts() / len(data_frame)))\n",
    "print(' ')\n",
    "print('Train set class ratio:')\n",
    "print('{}'.format(train_set[\"diagnosis\"].value_counts() / len(train_set)))\n",
    "print(' ')\n",
    "print('Test set class ratio:')\n",
    "print('{}'.format(test_set[\"diagnosis\"].value_counts() / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have labels, then you can just do a random split of the data.\n",
    "\n",
    "You need to make sure that the split data uses some kind of unique identifier for each sample, to ensure that multiple splits do not cause some of the data to appear in training (i.e. if more data is added to the set, then your random split might shuffle everything together).\n",
    "\n",
    "Benefits:\n",
    "- Each sample is \"assigned\" to test set based on some immutable value (the identifier), \n",
    "- You can specify the percentage of samples that go into testing vs. training (20%), \n",
    "- If you add new data, the samples that you had before will still be assigned to the correct split,\n",
    "- You can still set the random seed to ensure repeatability.\n",
    "\n",
    "If you don't have a unique identifier, then you can use the index -- just make sure that new data is added to the end of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions to calculate a hash for the dataset\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    '''Return a boolean that states whether the current sample should be included in the testing set.\n",
    "    \n",
    "    Calculates a hash value from an identifier, and returns True if the value is in the bottom \n",
    "    (test_ratio)-percent of the maximum possible hash value.\n",
    "    '''\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    '''Return training and testing dataframes given a test ratio and column to use for computing sample hash.\n",
    "    \n",
    "    Uses test_set_check to actually compute hash and put the data into training or testing.\n",
    "    '''\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "# Apply the above functions to the dataset\n",
    "train_set, test_set = split_train_test_by_id(data_frame, 0.3, \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data has been split appropriately\n",
    "print('Overall class ratio:')\n",
    "print('{}'.format(data_frame[\"diagnosis\"].value_counts() / len(data_frame)))\n",
    "print(' ')\n",
    "print('Train set class ratio:')\n",
    "print('{}'.format(train_set[\"diagnosis\"].value_counts() / len(train_set)))\n",
    "print(' ')\n",
    "print('Test set class ratio:')\n",
    "print('{}'.format(test_set[\"diagnosis\"].value_counts() / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "You can now start looking at the training data to try and identify some correlations between features.\n",
    "\n",
    "First create a copy of the data so you don't mess anything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = train_set.copy()\n",
    "\n",
    "# Drop the 'id' column for analysis\n",
    "data_copy = data_copy.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Feature Correlations\n",
    "\n",
    "You can calculate **standard correlation coefficient** a.k.a. **Pearson's r** to look for pairwise correlations.\n",
    "\n",
    "See [this page](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/) which illustrates how to use the `.corr()` function to actually drop correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in finding ALL correlated features, not just positively correlated ones\n",
    "corr_matrix = data_copy.corr().abs()\n",
    "\n",
    "# `corr_matrix` is a symmetric matrix, so we just want the upper triangle\n",
    "upper_triangle_locations = np.triu( np.ones(corr_matrix.shape), k=1).astype(np.bool)\n",
    "\n",
    "# `upper` now contains just the upper triangle of correlations, with the rest as NaNs\n",
    "upper = corr_matrix.where(upper_triangle_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get a list of columns in `upper` that contain feature values correlated above 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print('{} columns to drop: {}'.format(len(to_drop), to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually perform the drop\n",
    "data_copy = data_copy.drop(data_copy[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results \n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "We can visualize the data to see if anything pops out -- this is very much \"explore mode\".\n",
    "\n",
    "As an example, we can create a scatter matrix (e.g. \"facet plot\") to display possible correlations between attributes.\n",
    "This is most helpful for regression targets, when you have a numeric value you want to estimate from the others (we don't have this in the BCA dataset, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"radius_mean\", \"texture_mean\", \"compactness_mean\", \"fractal_dimension_mean\"]\n",
    "\n",
    "scatter_matrix(data_copy[attributes], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to zoom in on a particular pair of features that might be informative, we can select them specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.plot(kind=\"scatter\", x=\"radius_mean\", y=\"fractal_dimension_mean\", \n",
    "             alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Combinations\n",
    "\n",
    "We won't do this here, but combining attributes can be done to include additional features you may want to look at. \n",
    "If you have a regression target, you can re-calculate your correlation between the target and your new feature to see if the new feature is correlated as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "First we can separate out our targets -- don't want to transform those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_values = train_set.drop('diagnosis', axis=1)\n",
    "\n",
    "## NOTE: Using double-brackets here to make the result a dataframe and not a series\n",
    "# See here: https://github.com/ageron/handson-ml/issues/259\n",
    "data_labels = train_set[['diagnosis']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will outline different data cleaning approaches, and then explain how to use Pipelines to string these all together.\n",
    "\n",
    "### Handle Missing Attributes\n",
    "\n",
    "Options:\n",
    "- Drop data with missing attributes\n",
    "- Remove attributes that are not complete\n",
    "- Set missing values to some other value\n",
    "\n",
    "Refer back to the \"Hands On\" notes for information about these approaches -- we have all numeric, complete features here so we won't worry about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For imputing missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Attributes\n",
    "\n",
    "Again, we have options:\n",
    "\n",
    "- Use `OrdinalEncoder` if the categorical attributes are ordinal\n",
    "- Use `OneHotEncoder` if the categorical attributes are not ordinal\n",
    "\n",
    "And again, we don't have this situation here. Refer to \"Hands On\" for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Attributes\n",
    "\n",
    "Data should always be scaled. \n",
    "\n",
    "**Min-Max Scaling**: AKA *normalization*: values are shifted and rescaled to range from 0 to 1, by subtracting the min and dividing by the max - min. There is a `MinMaxScaler` transformer for this, with a `feature_range` hyperparameter.\n",
    "\n",
    "**Standardization**: Zero mean, unit variance. Subtract the mean, divide by standard deviation. Not bounded to any specific range, which may be a problem (e.g. for neural networks expecting a 0-1 value), but much less affected by outliers. `StandardScaler` will do this.\n",
    "\n",
    "Scaling should be calculated only on the training set, and the proper transform applied to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipelines\n",
    "\n",
    "Pipelines allow you to \"chain\" together processing of the data (imputer, encoder, scaler).\n",
    "In this case, since we aren't imputing and we aren't handling categorical values, we only have a scaling component to our pipeline, but this is a good way to set up a series of cleaning operations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "input_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# This does the same thing, adds a name automatically\n",
    "input_pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "data_values_transformed = input_pipeline.fit_transform(data_values)\n",
    "\n",
    "# Create a numeric label for our system to work on\n",
    "data_labels_num = ordinal_encoder.fit_transform(data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "These methods will give you a more \"all-encompassing\" view of your data by providing a low-dimensional embedding or representation of the data in 2 or 3 dimensions.\n",
    "\n",
    "You can add these methods to the pipeline as well, by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create pipelines for different dimensionality reduction targets. \n",
    "There may (?) be a way to define multiple alternatives for a given step, but I'm not sure.\n",
    "\n",
    "Here are some of the different methods you can try:\n",
    "\n",
    "```\n",
    "dimred_pipeline = make_pipeline(input_pipeline, PCA(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, Isomap(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, LocallyLinearEmbedding(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, MDS(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, TSNE(n_components=2))\n",
    "```\n",
    "\n",
    "Cut and paste whichever of those you want down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_pipeline = make_pipeline(input_pipeline, PCA(n_components=2))\n",
    "#dimred_pipeline = make_pipeline(input_pipeline, TSNE(n_components=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and apply the transform right away\n",
    "X_reduced = dimred_pipeline.fit_transform(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reduced-dimensional space (2D)\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reduced-dimensional space - 3D\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.scatter(X_reduced[:,0], X_reduced[:,1], X_reduced[:,2], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the dim reduction, with labels for the maps\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=np.squeeze(data_labels_num), alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.scatter(X_reduced[:,0], X_reduced[:,1], c=data_labels_num, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Clustering is for unlabeled data, where you can decide on a label just based on the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Clustering pipeline - Start to finish\n",
    "kmeans_pipeline = make_pipeline(input_pipeline, PCA(n_components=2), KMeans(n_clusters=2))\n",
    "\n",
    "# Cluster via K-means\n",
    "X_clustered = kmeans_pipeline.fit_predict(data_values)\n",
    "\n",
    "# Alternative: Create the input data first, then fit the model and transform separately\n",
    "kmeans_model = KMeans(n_clusters=2).fit(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=X_clustered, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "sc_pipeline = make_pipeline(dimred_pipeline, SpectralClustering(n_clusters=2, assign_labels='discretize'))\n",
    "data_reduced_sc = sc_pipeline.fit_predict(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=data_reduced_sc, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "ms_pipeline = make_pipeline(dimred_pipeline, MeanShift())\n",
    "data_reduced_ms = ms_pipeline.fit_predict(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=data_reduced_ms, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate, we have several metrics to choose from depending on whether or not we have ground truth labels.\n",
    "\n",
    "If we **DO** have the labels:\n",
    "- [Adjusted Rand index](https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-index)\n",
    "- [Mutual Information](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores)\n",
    "- [Homogeneity, Completeness, and V-measure](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)\n",
    "- [Fowlkes-Mallows Scores](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores)\n",
    "- [Contingency Matrix](https://scikit-learn.org/stable/modules/clustering.html#contingency-matrix)\n",
    "\n",
    "If we **DO NOT** have the labels:\n",
    "- [Silhouette Coefficient](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n",
    "- [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)\n",
    "- [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Uses Labels')\n",
    "print('===========')\n",
    "print('Adjusted Rand Index: \\t{:.3f}'.format(metrics.adjusted_rand_score(np.squeeze(data_labels_num), X_clustered)))\n",
    "print('Mutual Information: \\t{:.3f}'.format(metrics.adjusted_mutual_info_score(np.squeeze(data_labels_num), X_clustered, average_method='arithmetic')))\n",
    "print('Homogeneity Score: \\t{:.3f}'.format(metrics.homogeneity_score(np.squeeze(data_labels_num), X_clustered)))\n",
    "print('Completeness Score: \\t{:.3f}'.format(metrics.completeness_score(np.squeeze(data_labels_num), X_clustered)))\n",
    "print('Fowlkes-Mallows Score: \\t{:.3f}'.format(metrics.fowlkes_mallows_score(np.squeeze(data_labels_num), X_clustered)))\n",
    "print(' ')\n",
    "print('No Labels')\n",
    "print('=========')\n",
    "print('Silhouette Coefficient: \\t{:.3f}'.format(metrics.silhouette_score(X_reduced, X_clustered, metric='euclidean')))\n",
    "print('Calinski-Harabasz Index:\\t{:.3f}'.format(metrics.calinski_harabasz_score(X_reduced, X_clustered)))\n",
    "print('Davies-Bouldin Score: \\t\\t{:.3f}'.format(metrics.davies_bouldin_score(X_reduced, X_clustered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Once we have labels, we can turn our attention to classification -- this will allow us to assign labels to our testing set.\n",
    "\n",
    "We'll go through some common methods, training and calculating the evaluation performance for each of them using basic parameters.\n",
    "For details on modifying / optimizing these, see individual notebooks or the `scikit-learn` User Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(data_values_transformed, data_labels_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_training = data_values.iloc[:20]\n",
    "some_training_transformed = input_pipeline.transform(some_training)\n",
    "some_training_labels = data_labels.iloc[:5]\n",
    "some_training_labels_num = ordinal_encoder.transform(some_training_labels)\n",
    "\n",
    "print(\"Predictions:\", list(lin_reg.predict(some_training_transformed)))\n",
    "print(\"Labels:\", list(some_training_labels_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = lin_reg.predict(data_values_transformed)\n",
    "lin_mse = mean_squared_error(data_labels_num, predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"Linear Regressor Root Mean Squared Error: {:.3f}\".format(lin_rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(data_values_transformed, data_labels_num)\n",
    "\n",
    "# Make predictions\n",
    "predictions = tree_clf.predict(data_values_transformed)\n",
    "tree_mse = mean_squared_error(data_labels_num, predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(\"Tree Root Mean Squared Error: \", tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
