{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkG_jIHLORoO"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook illustrates basic data loading and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0J-bdoZKTrUH"
   },
   "source": [
    "These are nearly identicial to the functions I used to generate the figures in my previous lecture, but the display code for the plotting has been greatly simplifed (to save on space and complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prFrDpuEPO0q"
   },
   "source": [
    "## Understanding Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeR-qNA8TyFI"
   },
   "source": [
    "I don't expect that you will be familiar with the code below; the notebook format will try to explain everything that's going on. In addition, there are comments within the code itself that explain what's happening. Comments are green lines that start with a pound sign (`#`).\n",
    "\n",
    "You can also feel free to view and mess with this notebook -- your changes should not be saved, but you can play around with various settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeYqhb8KQKjy"
   },
   "source": [
    "# Imports and Project Setup\n",
    "\n",
    "We set up our project and import important packages here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kw_pfqH-T1Zd"
   },
   "source": [
    "Python requires the use of \"modules\", which are basically packages of code that enable us to use more advanced functionality than the core library. We don't want to have to write all the functions to load data, create plots, and train models by hand, so we load these modules at the start so we have access to pre-built sets of code that will do all this boring stuff for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SziUGzLTQjEt"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IHypnfVT4ux"
   },
   "source": [
    "Most of the packages we import are very common and popular for machine learning. Because they are so important, I will provide some links with more info for them, so you can explore their functionality on your own. \n",
    "\n",
    "- `pandas`: This module defines a table-like format for data called \"dataframes\"; these are similar to MATLAB's `table`, R's `dataframe`, or Excel spreadsheets. Here we'll use them for loading and parsing the data.\n",
    "- `numpy`: This is a very common module for doing numeric analysis. It provides support for matrices and tensors, as well as hundreds of mathematical operations and commonly-used linear algebra functions.\n",
    "- `matplotlib`: This is the \"matrix plotting library\", which allows us to easily generate plots and charts of our data. Thanks to the format of the notebook, these plots will appear directly in our web browser!\n",
    "\n",
    "The code cell below, when executed, will run the import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckabkLoR2BJc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We use two different plotting libraries, depending on which kind of plot we want\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set an option for Pandas to display smaller floating-point numbers\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Import libraries to work with strings\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v36wc1ErzRI"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuxX_vsmUIap"
   },
   "source": [
    "We typically load data from `.csv` files, which are basically spreadsheets.\n",
    "\n",
    "In Excel, you can export spreadsheets as `.csv` by going to \"File\" -> \"Export\" -> \"Change File Type\" and selecting **\"CSV (Comma delimited) (*.csv)**\" under \"Other File Types\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access in Local Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to perform data loading is to load it from the local workspace. If this notebook is being run locally, then the code below will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.path.join to construct the file path\n",
    "# Keeps things nice when going between Windows / non-Windows platforms\n",
    "csv_file = os.path.join('data', 'bca_wisconsin', 'bca_wisconsin.csv')\n",
    "\n",
    "# Use `read_csv` to get the file into a dataframe\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0i-D1--1Rfl2"
   },
   "source": [
    "## Data Access in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dPBYaokULvd"
   },
   "source": [
    "One area that Goole's Colab needs some work is in accessing data within the Google Drive. Currently (I think for security reasons), we have to go through the Google Drive API.\n",
    "\n",
    "This process will allow us to access the file which holds our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPkzcWYoPmpP"
   },
   "source": [
    "### Option 1: Mounting Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPgJrkIaPpnX"
   },
   "source": [
    "We can mount the Google Drive to the notebook's VM, and then load the csv file to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "w798QG-_7rQs",
    "outputId": "3c600522-7d1d-40c0-a898-9b2e9936ec0c"
   },
   "outputs": [],
   "source": [
    "# Need to get Google Drive access\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruhPXWOWUcuc"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas dataframe\n",
    "data_dir = os.path.join('/content/gdrive/My Drive/2020-tata-memorial-workshop/wisconsin_breast_cancer_data.csv')\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_aFY8KBP_TH"
   },
   "source": [
    "### Option 2: Uploading a File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkQjFnqgNZrO"
   },
   "source": [
    "Alternatively, you can use the `files.upload()` function to upload a file from your local computer. If you download the csv file, you can upload it to the notebook's VM using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Eq-gurZLNksi",
    "outputId": "e6f07cec-b507-421b-b095-2425294a48a2"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQPsL9fNPCvP"
   },
   "source": [
    "`uploaded` is now a dictionary where the key is the name of the file, and the value is a byte-formatted string. We can pass this through `StringIO` to obtain a csv-file-like object, and then load it with `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30xBwOoaN_BQ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(StringIO(uploaded['wisconsin_breast_cancer_data.csv'].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oo614rWqUOak"
   },
   "source": [
    "## Double-Check Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yck6w3uURya"
   },
   "source": [
    "It's a good idea to take a peek at what we've loaded, just to make sure that we don't have an empty or corrupted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "jjW3NIix6810",
    "outputId": "9a964f4f-df53-470d-cc15-ec4606a221b6"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aU0q--njUlCa"
   },
   "source": [
    "We can also use the `.info()` method to get a peek at each of the columns in our dataset and see what kind of values we have. We can compare the number of entries to the number of `non-null` values in each column to see whether we have any missing data, and we can check which values are integers, floating points (i.e. decimal places), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "VxEr28svrPmZ",
    "outputId": "92abbdf6-fc62-4b5a-e7ef-62323a62e9a7"
   },
   "outputs": [],
   "source": [
    "# View some statistics on the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_upek-P_GG-H"
   },
   "source": [
    "## Handle Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKmhEp8DXiaw"
   },
   "source": [
    "Generally, for machine learning, if you have a feature with categorical values (like \"Hot\" and \"Cold\"), you want to convert them to numeric values. There are two ways of doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klK-sL99WaKG"
   },
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4A0g7Xx2W6Ey"
   },
   "source": [
    "If the categorical values are **ordinal**, meaning you can place them in some kind of order (e.g. \"Low\", \"Intermediate\", and \"High\"), you can convert these into ordered numeric values where Low = 0, Intermediate = 1, and High = 2. In Python you can use the `OrdinalEncoder` package to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf339DMvtknJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbAgGO6qWg6u"
   },
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pkj_IJ_lW3uu"
   },
   "source": [
    "If the values are **not ordinal**, meaning the order of them doesn't matter (e.g. \"Blood Type A\", \"Blood Type B\") then you can use **one-hot encoding**: Replace the feature with $N$ new features, where $N$ is the number of categories. Each of the new features is *binary*, meaning it's only 0 or 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0o2WKYS7GKX-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhGaTjf8WqQ3"
   },
   "source": [
    "Luckily, we don't have this situation here in our feature set: All of our measured values are floating-point values. The only categorical entry in our data is our target categories. Targets (or classes) are often encoded numerically, so let's do that now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yi3lq9A_Ck9s"
   },
   "source": [
    "### Converting Target to Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tWTKT81XX3W"
   },
   "source": [
    "Our targets in this dataset are encoded as characters, \"M\" (standing for \"Malignant\") and \"B\" (standing for \"Benign\"). Practically speaking, it's easier to work with these labels if they are numeric.\n",
    "\n",
    "We have what's called a **binary class problem**, meaning that there are only two categories of data that we need to worry about. For this type of problem, it's common to encode the categories as 0 and 1. \n",
    "\n",
    "In our case, we're going to set \"Benign\" to 0 and \"Malignant\" to 1 (there's no technical reason for this; to me, it makes sense because 1 is typically referred to as \"positive\", much like a \"positive\" diagnosis of malignancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCGOXM9iDvco"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "diagnosis_cat = df['diagnosis']\n",
    "\n",
    "# Fit the encoder to the categories, and immediately \n",
    "diagnosis_lab = label_encoder.fit_transform(diagnosis_cat)\n",
    "\n",
    "# Add the diagnosis label back to the dataframe\n",
    "df['diagnosis_label'] = diagnosis_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "B92ukUhRHts1",
    "outputId": "ab04ba7c-f97f-46b8-ed2b-b16ee9e7e466"
   },
   "outputs": [],
   "source": [
    "# Ensure the labels were added correctly\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHgri99jr_AG"
   },
   "source": [
    "# Training and Testing Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1iaUgeRXvUK"
   },
   "source": [
    "The order of data wrangling techniques is debatable, but number one is **separate out a testing set**.\n",
    "\n",
    "Separating out a training and testing set is a fundamental step of good machine learning. During data exploration, it helps to prevent **data snooping bias**, which can influence your design decisions. During training, it helps prevent **overfitting** your model to your specific data, thus improving **generalization**. \n",
    "\n",
    "A good general rule is that around **60-70%** of your dataset should be set aside for training, and the remaining **40-30%** should be used for performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRsCgXLvsBVW"
   },
   "source": [
    "## Splitting Strategies: Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUB_HxcaXyqi"
   },
   "source": [
    "How do we identify which samples go in which split? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3d2ETnAdiDX"
   },
   "source": [
    "### Na&iuml;ve Random Sampling\n",
    "\n",
    "We *could* start by just saying that we'll take a random sample of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3erH4wvbgrF"
   },
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio=0.3):\n",
    "    \"\"\"Return a random split of the \"data\" dataframe, with the percentage of \n",
    "    data specified in \"test_ratio\" in the testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    # First get a random list of indices into the data\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    \n",
    "    # Calculate the number of indices that belong to the test set \n",
    "    # (e.g. 20% if test_ratio is 0.2)\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "\n",
    "    # Split the random list of indices into two sets, one for test indices and \n",
    "    # another for training indices \n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "    # Return two dataframes, the first with the training data and the other \n",
    "    # with the testing data \n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LuCbl2nbaH_"
   },
   "source": [
    "However, this doesn't quite work: If I run this code twice, I will get two different datasets, because the `np.random.permutation()` method will run two different times. This means that in Run 2, I may have some data in my training set that was previously in my testing set, and vice versa. \n",
    "\n",
    "You can prevent this by setting a \"random seed\", meaning that each run will give you the *same* random numbers -- but if more data is added to the set, then even using the same seed will give you a completely different random split.\n",
    "\n",
    "**This is no good!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6Wz-FYKdl1L"
   },
   "source": [
    "### Indexed Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2GvTMIpZNea"
   },
   "source": [
    "A better strategy is to use some kind of unique identifier for each sample, and based on that number, place it into either the training or the testing set. \n",
    "\n",
    "Benefits:\n",
    "- Each sample is \"assigned\" to test set based on some immutable value (the identifier);\n",
    "- You can specify the percentage of samples that go into testing vs. training (20%);\n",
    "- If you add new data, the samples that you had before will still be assigned to the correct split; and\n",
    "- You can still set the random seed to ensure repeatability.\n",
    "\n",
    "If you don't have a unique identifier, then you can use the index -- just make sure that new data is added to the end of the dataset. \n",
    "\n",
    "Luckily, we **do** have a unique identifier, a numeric value assigned to each subject in the database. So we'll just use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kBvyq1Bd2VC"
   },
   "outputs": [],
   "source": [
    "# Import functions to calculate a hash for the dataset\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    '''Return a boolean that states whether the current sample should be included in the testing set.\n",
    "    \n",
    "    Calculates a hash value from an identifier, and returns True if the value is in the bottom \n",
    "    (test_ratio)-percent of the maximum possible hash value.\n",
    "    '''\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    '''Return training and testing dataframes given a test ratio and column to use for computing sample hash.\n",
    "    \n",
    "    Uses test_set_check to actually compute hash and put the data into training or testing.\n",
    "    '''\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "# Apply the above functions to the dataset\n",
    "train_set, test_set = split_train_test_by_id(df, 0.3, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "dfVJ9pfKw9ZY",
    "outputId": "c38ac4e1-9c9f-4b45-e0e1-b8697541c995"
   },
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28sgQDfDfo8a"
   },
   "source": [
    "Note: If you don't understand this code, don't worry about it -- it's a bit more complex than most of what we're doing, and the implementation details aren't important. Basically, it says \"Look at the subject's ID number, and based on that number, either put it into the test set or the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9H9rrTYjdto"
   },
   "source": [
    "This works pretty well, and if you don't know the labels / targets / classes of your dataset, this is the best option. \n",
    "\n",
    "However, we **do** have labels for our data. So we'd like to know whether the **class balance** of the overall dataset matches the training and / or testing splits. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "q3YsvV8PsXUw",
    "outputId": "59684353-b4f9-4dea-da90-74655f67d32e"
   },
   "outputs": [],
   "source": [
    "print('================')\n",
    "print(' Random Sampling')\n",
    "print('================')\n",
    "print('')\n",
    "print('Overall class balance:')\n",
    "print('{}'.format(df[\"diagnosis\"].value_counts() / len(df)))\n",
    "print(' ')\n",
    "print('Train set class ratio:')\n",
    "print('{}'.format(train_set[\"diagnosis\"].value_counts() / len(train_set)))\n",
    "print(' ')\n",
    "print('Test set class ratio:')\n",
    "print('{}'.format(test_set[\"diagnosis\"].value_counts() / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4e6Ma2o5jxbw"
   },
   "source": [
    "The ratios are different between each of the splits, and both of them differ from the overall dataset. This effect is exaggerated when you have a small number of samples. How can we fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zba_T9eRaez3"
   },
   "source": [
    "## Splitting Strategies: Stratified Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5ldIkwyhbrA"
   },
   "source": [
    "How do you **balance** the samples in your dataset while maintaining randomness?\n",
    "\n",
    "**Stratified Sampling** maintains the overall label distribution in your training and testing sets -- this ensures that your training and testing sets accurately represent the class distribution in the overall dataset.\n",
    "\n",
    "Because this is such a common thing to do, there is a built-in function to `sklearn` that will do it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YE1W3JjmsD9G"
   },
   "outputs": [],
   "source": [
    "# Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create the splitting object\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply the split to the data frame using the \"diagnosis\" column as our label\n",
    "for train_index, test_index in split.split(df, df[\"diagnosis\"]):\n",
    "    train_set = df.loc[train_index]\n",
    "    test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImMVLZZ4kPx5"
   },
   "source": [
    "The `StratifiedShuffleSplit()` function allows you to define the number of splits, the size of the testing set, and the random seed to use for reproducibility. You then call the `split()` method on the splitting object, and give it the object to split (our dataframe) and the thing to use to perform the split (our class labels, i.e. the \"diagnosis\" column).\n",
    "\n",
    "Let's take a look at the class balance now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "3xhICVLYsJ21",
    "outputId": "475e42e4-487a-4587-82ba-cb505ec610c8"
   },
   "outputs": [],
   "source": [
    "print('====================')\n",
    "print(' Stratified Sampling')\n",
    "print('====================')\n",
    "print('')\n",
    "print('Overall class ratio:')\n",
    "print('{}'.format(df[\"diagnosis\"].value_counts() / len(df)))\n",
    "print(' ')\n",
    "print('Train set class ratio:')\n",
    "print('{}'.format(train_set[\"diagnosis\"].value_counts() / len(train_set)))\n",
    "print(' ')\n",
    "print('Test set class ratio:')\n",
    "print('{}'.format(test_set[\"diagnosis\"].value_counts() / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mte8ORM_k1DU"
   },
   "source": [
    "This is much more balanced: Both our training and testing sets have the same ratio of benign to malignant samples, and they are also close to the overall class ratio. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVfByvG8qvOD"
   },
   "source": [
    "\n",
    "### Side Note: Balanced vs. Unbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IaHd03UNqwD5"
   },
   "source": [
    "The ratio of benign to malignant classes is not 50-50, but is closer to 60-40. This is very common in biomedical datasets, where disease cases or rare classes are by definition very small percentages of the overall data.\n",
    "\n",
    "A common theme in machine learning is the tension between needing enough data to build a model, but most of the phenomena we're interested in are comparatively rare. It will be up to you (or your data scientist) to select the best method for the amount of data you have available, and to adjust your evaluation metrics accordingly. \n",
    "\n",
    "If necessary, you can **over-sample** or **under-sample** a class to try to achieve an even split. However, in our case, we're going to leave the class imbalance in place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1yjHZALvmF6"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ue3YzJ8YTlz9"
   },
   "source": [
    "Very often, the data you receive will be \"messy\" -- meaning there will be **missing values**, **categorical** rather than numeric features, and values that **need to be scaled**. \n",
    "\n",
    "We are NOT considering cases where values may be incorrectly entered, for two reasons:\n",
    "\n",
    "1. Detecting \"outliers\" is an entire lecture on its own, with a variety of different approaches that all have pros and cons. \n",
    "2. It's possible that a value can be incorrect, but still within a reasonable range -- these are not outliers, but they are wrong. We **must assume** that the data is correctly entered into the spreadsheet, and that there are protections in place to catch incorrectly-entered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVVhGnJcTigL"
   },
   "source": [
    "First we can separate out our targets -- don't want to transform those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOG1WXnZF-Is"
   },
   "outputs": [],
   "source": [
    "training_values = train_set.drop(['id','diagnosis', 'diagnosis_label'], axis=1)\n",
    "\n",
    "## NOTE: Using double-brackets here to make the result a dataframe and not a series\n",
    "# See here: https://github.com/ageron/handson-ml/issues/259\n",
    "training_labels = train_set[['diagnosis_label']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFAJKC0-F-ln"
   },
   "source": [
    "First I will outline different data cleaning approaches, and then explain how to use Pipelines to string these all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkEJQjlXtMeT"
   },
   "source": [
    "### Handle Missing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpkM9RggtOz3"
   },
   "source": [
    "Options:\n",
    "- Drop data with missing attributes\n",
    "- Remove attributes that are not complete\n",
    "- Set missing values to some other value\n",
    "\n",
    "Refer back to the \"Hands On\" notes for information about these approaches -- we have all numeric, complete features here so we won't worry about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsaiVh3KGA7X"
   },
   "outputs": [],
   "source": [
    "# For imputing missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HZgJTyOF5Cg"
   },
   "source": [
    "### Scaling Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSr79etMtWhD"
   },
   "source": [
    "Data should always be scaled. \n",
    "\n",
    "**Min-Max Scaling**: AKA *normalization*: values are shifted and rescaled to range from 0 to 1, by subtracting the min and dividing by the max - min. There is a `MinMaxScaler` transformer for this, with a `feature_range` hyperparameter.\n",
    "\n",
    "**Standardization**: Zero mean, unit variance. Subtract the mean, divide by standard deviation. Not bounded to any specific range, which may be a problem (e.g. for neural networks expecting a 0-1 value), but much less affected by outliers. `StandardScaler` will do this.\n",
    "\n",
    "Scaling should be calculated only on the training set, and the proper transform applied to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf6zSYpcGTwl"
   },
   "source": [
    "### Using Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQfWLtATtaL-"
   },
   "source": [
    "Pipelines allow you to \"chain\" together processing of the data (imputer, encoder, scaler).\n",
    "In this case, since we aren't imputing and we aren't handling categorical values, we only have a scaling component to our pipeline, but this is a good way to set up a series of cleaning operations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cJMW6yqGYY2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "input_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# This does the same thing, adds a name automatically\n",
    "input_pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "training_values_transformed = input_pipeline.fit_transform(training_values)\n",
    "\n",
    "# Create a numeric label for our system to work on\n",
    "#training_labels_num = ordinal_encoder.fit_transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwXL6W9DW_RC"
   },
   "source": [
    " # Data Exploration and Visualization\n",
    "\n",
    "In this section, we start looking at the training data to try and identify some patterns and correlations between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SB8L_Tc-UWDV"
   },
   "source": [
    "First create a copy of the data so you don't mess anything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOrmtsDQsbtV"
   },
   "outputs": [],
   "source": [
    "data_copy = train_set.copy()\n",
    "\n",
    "# Drop the 'id' and 'diagnosis' columns for analysis\n",
    "data_copy = data_copy.drop(['id', 'diagnosis', 'diagnosis_label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaDCCKnrsgFV"
   },
   "source": [
    "### Calculating Feature Correlations\n",
    "\n",
    "You can calculate **standard correlation coefficient** a.k.a. **Pearson's r** to look for pairwise correlations.\n",
    "\n",
    "See [this page](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/) which illustrates how to use the `.corr()` function to actually drop correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "sCCCC7PJs6-l",
    "outputId": "09427f34-2e8a-4b2a-93e6-e00911f9bb28"
   },
   "outputs": [],
   "source": [
    "# We are interested in finding ALL correlated features, not just positively correlated ones\n",
    "corr_matrix = data_copy.corr().abs()\n",
    "\n",
    "# `corr_matrix` is a symmetric matrix, so we just want the upper triangle\n",
    "upper_triangle_locations = np.triu( np.ones(corr_matrix.shape), k=1).astype(np.bool)\n",
    "\n",
    "# `upper` now contains just the upper triangle of correlations, with the rest as NaNs\n",
    "upper = corr_matrix.where(upper_triangle_locations)\n",
    "\n",
    "# Now get a list of columns in `upper` that contain feature values correlated above 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print('{} columns to drop: {}'.format(len(to_drop), to_drop))\n",
    "\n",
    "# Actually perform the drop\n",
    "data_copy = data_copy.drop(data_copy[to_drop], axis=1)\n",
    "\n",
    "# Display the results \n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyWD0yFYtHWq"
   },
   "source": [
    "## Feature Selection and Visualization\n",
    "\n",
    "We can visualize the data to see if anything pops out -- this is very much \"explore mode\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGJrDekSW-8w"
   },
   "source": [
    "### Univariate Data Visualization: Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "zV1Elz_F7gy1",
    "outputId": "f9991622-0980-4e77-8847-a9597280548f"
   },
   "outputs": [],
   "source": [
    "# Create a histogram of a single feature\n",
    "plt.hist(data_copy['radius_mean'], density=True)\n",
    "plt.title('Radius Mean for All Samples')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95ecowNA9KEx"
   },
   "outputs": [],
   "source": [
    "# Separate the data into classes for easier plotting\n",
    "malignant = data_copy[train_set['diagnosis_label'] == 1]\n",
    "benign = data_copy[train_set['diagnosis_label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "umT1aztV91ah",
    "outputId": "35881ecf-8249-4099-fa08-bf949a497ec0"
   },
   "outputs": [],
   "source": [
    "# Create data plots\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x1 = 'texture_mean'\n",
    "x1_display = x1.replace('_', ' ').title()\n",
    "\n",
    "\n",
    "# These two lines do the work of plotting a histogram\n",
    "ax.hist(malignant[x1], density=True, alpha=.8, label=\"Malignant\")\n",
    "ax.hist(benign[x1], density=True, alpha=.8, label=\"Benign\")\n",
    "\n",
    "\n",
    "ax.set(xlim=(5,35))\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x$: '+x1_display,\n",
    "       ylabel=r'$p(x|\\omega_{j})$',\n",
    "       title='Probability Density Function (PDF) for '+x1_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "KzS2WBuBlgV2",
    "outputId": "ae8c1fe4-6ce3-4281-db52-5f7e9348ece5"
   },
   "outputs": [],
   "source": [
    "# Create data plots\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x1 = 'radius_mean'\n",
    "x1_display = x1.replace('_', ' ').title()\n",
    "\n",
    "ax.hist(malignant[x1], density=True, alpha=.8, label=\"Malignant\")\n",
    "ax.hist(benign[x1], density=True, alpha=.8, label=\"Benign\")\n",
    "ax.set(xlim=(5,35))\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x$: '+x1_display,\n",
    "       ylabel=r'$p(x|\\omega_{j})$',\n",
    "       title='Probability Density Function (PDF) for '+x1_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhUnZccBXJrM"
   },
   "source": [
    "### Multivariate Data Visualization: Scatter Plots and 2D Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "YyuNemL-XOBv",
    "outputId": "727e054b-fc64-4005-c541-74015831e72d"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "x1 = 'radius_mean'\n",
    "x2 = 'texture_mean'\n",
    "\n",
    "x1_display = x1.replace('_', ' ').title()\n",
    "x2_display = x2.replace('_', ' ').title()\n",
    "\n",
    "ax.scatter(malignant[x1], malignant[x2], alpha=.8, label=\"Malignant\")\n",
    "ax.scatter(benign[x1], benign[x2], alpha=.8, label=\"Benign\")\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x_{1}$: '+x1_display,\n",
    "       ylabel=r'$x_{2}$: '+x2_display,\n",
    "       title=x1_display+' vs. '+x2_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEs7gj6tmORR"
   },
   "source": [
    "### Viewing More than 2 or 3 Variables with Facet Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZEG-0tGuBmn"
   },
   "source": [
    "As an example, we can create a scatter matrix (e.g. \"facet plot\") to display possible correlations between attributes.\n",
    "This is most helpful for regression targets, when you have a numeric value you want to estimate from the others (we don't have this in the BCA dataset, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "c0tRAL2VtOKk",
    "outputId": "985a972c-283b-4b7e-b199-c5a419efd947"
   },
   "outputs": [],
   "source": [
    "attributes = [\"radius_mean\", \"texture_mean\", \"compactness_mean\", \"fractal_dimension_mean\"]\n",
    "\n",
    "# We need to add the \"diagnosis\" label back in here, so Seaborn can plot it using the `hue` parameter\n",
    "data_copy_display = data_copy[attributes].copy()\n",
    "data_copy_display['diagnosis'] = train_set['diagnosis']\n",
    "\n",
    "g = sns.pairplot(data_copy_display, hue='diagnosis', plot_kws={'alpha': 0.5, 'edgecolor': None}, height=3, aspect=1)\n",
    "\n",
    "# Alter the plot\n",
    "g.fig.suptitle('Pair Plot of '+str(len(attributes)-1)+' Features', y=1.02)\n",
    "g._legend.set_title(\"Diagnosis\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I35R8Mc9tXH4"
   },
   "source": [
    "If we want to zoom in on a particular pair of features that might be informative, we can select them specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "IkeyCGZHmGI6",
    "outputId": "0db6adce-515f-4f2b-a621-cfabb5ed12f1"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x1 = 'fractal_dimension_mean'\n",
    "x2 = 'radius_mean'\n",
    "\n",
    "x1_display = x1.replace('_', ' ').title()\n",
    "x2_display = x2.replace('_', ' ').title()\n",
    "\n",
    "ax.scatter(malignant[x1], malignant[x2], alpha=.8, label=\"Malignant\")\n",
    "ax.scatter(benign[x1], benign[x2], alpha=.8, label=\"Benign\")\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x_{1}$: '+x1_display,\n",
    "       ylabel=r'$x_{2}$: '+x2_display,\n",
    "       title=x1_display+' vs. '+x2_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pizH8T8Etewk"
   },
   "source": [
    "### Attribute Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5L_Va75as1Gw"
   },
   "source": [
    "We won't do this here, but combining attributes can be done to include additional features you may want to look at. \n",
    "If you have a regression target, you can re-calculate your correlation between the target and your new feature to see if the new feature is correlated as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcZwGaWmGarV"
   },
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCZ4SeGItefW"
   },
   "source": [
    "These methods will give you a more \"all-encompassing\" view of your data by providing a low-dimensional embedding or representation of the data in 2 or 3 dimensions.\n",
    "\n",
    "You can add these methods to the pipeline as well, by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PnH3HslGeQE"
   },
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cF1fU6auGgp9"
   },
   "source": [
    "Now create pipelines for different dimensionality reduction targets. \n",
    "There may (?) be a way to define multiple alternatives for a given step, but I'm not sure.\n",
    "\n",
    "Here are some of the different methods you can try:\n",
    "\n",
    "```\n",
    "dimred_pipeline = make_pipeline(input_pipeline, PCA(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, Isomap(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, LocallyLinearEmbedding(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, MDS(n_components=2))\n",
    "dimred_pipeline = make_pipeline(input_pipeline, TSNE(n_components=2))\n",
    "```\n",
    "\n",
    "Cut and paste whichever of those you want down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5h5Ec8rPGip3"
   },
   "outputs": [],
   "source": [
    "dimred_pipeline = make_pipeline(input_pipeline, PCA(n_components=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8dpyWwFuyuZ"
   },
   "outputs": [],
   "source": [
    "# Fit and apply the transform right away\n",
    "X_reduced = dimred_pipeline.fit_transform(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "oj8NHnp7uiBO",
    "outputId": "efa8ec9a-de3f-42b3-8681-5db6f2bcd1da"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax.scatter(X_reduced[:,0], X_reduced[:,1], alpha=.8, label=\"Unlabeled\")\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x_{1}$',\n",
    "       ylabel=r'$x_{2}$',\n",
    "       title=r'Reduced Dimensional Space (Unlabeled)')\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "ibfBxsyVuA-r",
    "outputId": "e1dc3f0c-d2d7-45c2-c13f-ff4076b27067"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Separate the data into classes for easier plotting\n",
    "malignant = X_reduced[train_set['diagnosis_label'] == 1]\n",
    "benign = X_reduced[train_set['diagnosis_label'] == 0]\n",
    "\n",
    "ax.scatter(malignant[:,0], malignant[:,1], alpha=.8, label=\"Malignant\")\n",
    "ax.scatter(benign[:,0], benign[:,1], alpha=.8, label=\"Benign\")\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x_{1}$',\n",
    "       ylabel=r'$x_{2}$',\n",
    "       title=r'Reduced Dimensional Space')\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PkG_jIHLORoO",
    "Cbuv-xCrPK-4",
    "prFrDpuEPO0q",
    "AeYqhb8KQKjy",
    "SziUGzLTQjEt",
    "_v36wc1ErzRI",
    "0i-D1--1Rfl2",
    "nPkzcWYoPmpP",
    "n_aFY8KBP_TH",
    "Oo614rWqUOak",
    "_upek-P_GG-H",
    "klK-sL99WaKG",
    "cbAgGO6qWg6u",
    "Yi3lq9A_Ck9s",
    "AHgri99jr_AG",
    "yRsCgXLvsBVW",
    "g3d2ETnAdiDX",
    "q6Wz-FYKdl1L",
    "Zba_T9eRaez3",
    "VVfByvG8qvOD",
    "Z1yjHZALvmF6",
    "mkEJQjlXtMeT",
    "7HZgJTyOF5Cg",
    "Bf6zSYpcGTwl",
    "dwXL6W9DW_RC",
    "uaDCCKnrsgFV",
    "pyWD0yFYtHWq",
    "VGJrDekSW-8w",
    "ZhUnZccBXJrM",
    "kEs7gj6tmORR",
    "pizH8T8Etewk",
    "IcZwGaWmGarV"
   ],
   "name": "01-data-exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
